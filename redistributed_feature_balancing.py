"""
Redistributed Feature Balancing for Smartphone Addiction Predictor
================================================================

This script creates a model where Screen_Time_Ratio importance (30.3%) is redistributed
to other features instead of being a separate feature. This creates better balance
across all features while maintaining the reduced Screen_Time dominance.

Author: AI Assistant
Date: 2025
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import LabelEncoder
import joblib

# Set style for better plots
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

print("üîç REDISTRIBUTED FEATURE BALANCING FOR SMARTPHONE ADDICTION PREDICTOR")
print("=" * 70)

# Load and prepare data
print("\nüìä Step 1: Loading and preparing data...")
df = pd.read_csv('phone_usage_india.csv')

# Clean column names
df.columns = df.columns.str.replace(' ', '_').str.replace('(', '').str.replace(')', '').str.replace('/', '_')

# Create target variable
df['addiction_label'] = (df['Screen_Time_hrs_day'] > 8).astype(int)

# Encode categorical variables
df['Gender_encoded'] = LabelEncoder().fit_transform(df['Gender'])
df['Location_encoded'] = LabelEncoder().fit_transform(df['Location'])

print(f"Dataset shape: {df.shape}")
print(f"Target distribution: {df['addiction_label'].value_counts().to_dict()}")

# FEATURE ENGINEERING: Create features without Screen_Time_Ratio
print("\nüîß Step 2: Creating redistributed features...")

# Original features
original_features = [
    'Age', 'Gender_encoded', 'Location_encoded', 'Screen_Time_hrs_day',
    'Data_Usage_GB_month', 'Calls_Duration_mins_day', 'Number_of_Apps_Installed',
    'Social_Media_Time_hrs_day', 'Streaming_Time_hrs_day', 'Gaming_Time_hrs_day',
    'Monthly_Recharge_Cost_INR'
]

# Create engineered features (excluding Screen_Time_Ratio)
df['Total_Entertainment_Time'] = df['Social_Media_Time_hrs_day'] + df['Streaming_Time_hrs_day'] + df['Gaming_Time_hrs_day']
df['Data_Usage_per_Hour'] = df['Data_Usage_GB_month'] / (df['Screen_Time_hrs_day'] * 30 + 1)
df['App_Density'] = df['Number_of_Apps_Installed'] / 100.0
df['Cost_per_Hour'] = df['Monthly_Recharge_Cost_INR'] / (df['Screen_Time_hrs_day'] * 30 + 1)
df['Communication_Intensity'] = df['Calls_Duration_mins_day'] / 60.0
df['Age_Group'] = pd.cut(df['Age'], bins=[0, 25, 35, 50, 100], labels=[0, 1, 2, 3]).astype(int)

# Create interaction features that boost other features
df['Screen_Time_x_Apps'] = df['Screen_Time_hrs_day'] * df['App_Density']
df['Entertainment_x_Data'] = df['Total_Entertainment_Time'] * df['Data_Usage_GB_month']
df['Age_x_Screen_Time'] = df['Age'] * df['Screen_Time_hrs_day']
df['Data_Usage_x_Apps'] = df['Data_Usage_GB_month'] * df['App_Density']
df['Entertainment_x_Cost'] = df['Total_Entertainment_Time'] * df['Monthly_Recharge_Cost_INR']

# Enhanced feature set (20 features instead of 21)
redistributed_features = [
    'Age', 'Gender_encoded', 'Location_encoded', 
    'Screen_Time_hrs_day',  # Keep original but will be weighted down
    'Data_Usage_GB_month', 'Data_Usage_per_Hour',
    'Calls_Duration_mins_day', 'Communication_Intensity',
    'Number_of_Apps_Installed', 'App_Density',
    'Social_Media_Time_hrs_day', 'Streaming_Time_hrs_day', 'Gaming_Time_hrs_day',
    'Total_Entertainment_Time',
    'Monthly_Recharge_Cost_INR', 'Cost_per_Hour',
    'Age_Group',
    'Screen_Time_x_Apps', 'Entertainment_x_Data', 'Age_x_Screen_Time',
    'Data_Usage_x_Apps', 'Entertainment_x_Cost'  # Additional interaction features
]

print(f"Original features: {len(original_features)}")
print(f"Redistributed features: {len(redistributed_features)}")

# Prepare data
X_original = df[original_features]
X_redistributed = df[redistributed_features]
y = df['addiction_label']

# Split data
X_train_orig, X_test_orig, y_train, y_test = train_test_split(X_original, y, test_size=0.2, random_state=42, stratify=y)
X_train_redist, X_test_redist, y_train, y_test = train_test_split(X_redistributed, y, test_size=0.2, random_state=42, stratify=y)

print("\nü§ñ Step 3: Training models with redistributed features...")

# Define models
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'SVM': SVC(probability=True, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)
}

# Define scalers
scalers = {
    'StandardScaler': StandardScaler(),
    'MinMaxScaler': MinMaxScaler(),
    'RobustScaler': RobustScaler()
}

results = {}
feature_importances = {}

# Test different combinations
for model_name, model in models.items():
    print(f"\nüîç Testing {model_name}...")
    
    for scaler_name, scaler in scalers.items():
        print(f"  üìä With {scaler_name}...")
        
        # Scale features
        X_train_scaled = scaler.fit_transform(X_train_redist)
        X_test_scaled = scaler.transform(X_test_redist)
        
        # Train model
        model.fit(X_train_scaled, y_train)
        
        # Make predictions
        y_pred = model.predict(X_test_scaled)
        accuracy = accuracy_score(y_test, y_pred)
        
        # Get feature importance (if available)
        if hasattr(model, 'feature_importances_'):
            importance_df = pd.DataFrame({
                'Feature': redistributed_features,
                'Importance': model.feature_importances_
            }).sort_values('Importance', ascending=False)
        elif hasattr(model, 'coef_'):
            # For linear models, use absolute coefficients
            coef_abs = np.abs(model.coef_[0])
            importance_df = pd.DataFrame({
                'Feature': redistributed_features,
                'Importance': coef_abs / coef_abs.sum()
            }).sort_values('Importance', ascending=False)
        else:
            importance_df = None
        
        # Store results
        key = f"{model_name} + {scaler_name}"
        results[key] = {
            'accuracy': accuracy,
            'model': model,
            'scaler': scaler,
            'feature_importance': importance_df
        }
        
        if importance_df is not None:
            screen_time_importance = importance_df[importance_df['Feature'] == 'Screen_Time_hrs_day']['Importance'].iloc[0] if 'Screen_Time_hrs_day' in importance_df['Feature'].values else 0
            print(f"    ‚úÖ Accuracy: {accuracy:.3f}, Screen_Time importance: {screen_time_importance:.1%}")

print("\nüìä Step 4: Analyzing redistributed results...")

# Find the best balanced model
best_models = []
for key, result in results.items():
    if result['feature_importance'] is not None:
        screen_time_importance = result['feature_importance'][result['feature_importance']['Feature'] == 'Screen_Time_hrs_day']['Importance'].iloc[0] if 'Screen_Time_hrs_day' in result['feature_importance']['Feature'].values else 0
        if 0.6 <= screen_time_importance <= 0.8:  # Target range
            best_models.append((key, result['accuracy'], screen_time_importance))

# Sort by accuracy
best_models.sort(key=lambda x: x[1], reverse=True)

print(f"\nüéØ Best redistributed models (Screen_Time 60-80%):")
for i, (key, accuracy, screen_time_imp) in enumerate(best_models[:3]):
    print(f"{i+1}. {key}: Accuracy={accuracy:.3f}, Screen_Time={screen_time_imp:.1%}")

# If no model in target range, find the closest
if not best_models:
    print("\n‚ö†Ô∏è No model in target range. Finding closest...")
    closest_models = []
    for key, result in results.items():
        if result['feature_importance'] is not None:
            screen_time_importance = result['feature_importance'][result['feature_importance']['Feature'] == 'Screen_Time_hrs_day']['Importance'].iloc[0] if 'Screen_Time_hrs_day' in result['feature_importance']['Feature'].values else 0
            distance_from_target = abs(screen_time_importance - 0.75)  # Target 75%
            closest_models.append((key, result['accuracy'], screen_time_importance, distance_from_target))
    
    closest_models.sort(key=lambda x: x[3])  # Sort by distance from target
    best_models = [(key, acc, imp) for key, acc, imp, _ in closest_models[:3]]

# Select the best model
if best_models:
    best_key = best_models[0][0]
    best_result = results[best_key]
    print(f"\nüèÜ Selected best redistributed model: {best_key}")
    print(f"   Accuracy: {best_result['accuracy']:.3f}")
    
    # Get feature importance
    importance_df = best_result['feature_importance']
    screen_time_importance = importance_df[importance_df['Feature'] == 'Screen_Time_hrs_day']['Importance'].iloc[0] if 'Screen_Time_hrs_day' in importance_df['Feature'].values else 0
    print(f"   Screen_Time importance: {screen_time_importance:.1%}")

# Create comparison visualization
print("\nüìà Step 5: Creating redistributed visualizations...")

# Compare top models
fig, axes = plt.subplots(2, 2, figsize=(16, 12))
axes = axes.ravel()

# Get top 4 models for comparison
top_models = sorted(results.items(), key=lambda x: x[1]['accuracy'], reverse=True)[:4]

for i, (key, result) in enumerate(top_models):
    if result['feature_importance'] is not None:
        ax = axes[i]
        top_features = result['feature_importance'].head(8)
        
        bars = ax.barh(range(len(top_features)), top_features['Importance'], 
                       color=plt.cm.viridis(np.linspace(0, 1, len(top_features))))
        
        ax.set_yticks(range(len(top_features)))
        ax.set_yticklabels(top_features['Feature'], fontsize=9)
        ax.set_xlabel('Feature Importance')
        ax.set_title(f'{key}\nAccuracy: {result["accuracy"]:.3f}')
        ax.grid(axis='x', alpha=0.3)
        
        # Add value labels
        for j, (bar, value) in enumerate(zip(bars, top_features['Importance'])):
            ax.text(value + 0.01, j, f'{value:.1%}', va='center', fontsize=8)

plt.tight_layout()
plt.savefig('redistributed_model_comparison.png', dpi=300, bbox_inches='tight')
plt.show()

# Before/After comparison with original model
print("\nüìä Step 6: Before/After redistributed comparison...")

# Train original model for comparison
original_model = RandomForestClassifier(n_estimators=100, random_state=42)
original_model.fit(X_train_orig, y_train)
original_importance = pd.DataFrame({
    'Feature': original_features,
    'Importance': original_model.feature_importances_
}).sort_values('Importance', ascending=False)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# Before (Original)
original_top = original_importance.head(6)
bars1 = ax1.barh(range(len(original_top)), original_top['Importance'], 
                 color='red', alpha=0.7)
ax1.set_yticks(range(len(original_top)))
ax1.set_yticklabels(original_top['Feature'])
ax1.set_xlabel('Feature Importance')
ax1.set_title('BEFORE: Original Random Forest\nScreen_Time dominance: 98.4%')
ax1.grid(axis='x', alpha=0.3)

for i, (bar, value) in enumerate(zip(bars1, original_top['Importance'])):
    ax1.text(value + 0.01, i, f'{value:.1%}', va='center', fontsize=10)

# After (Best Redistributed Model)
if best_models:
    best_importance = best_result['feature_importance']
    best_top = best_importance.head(6)
    bars2 = ax2.barh(range(len(best_top)), best_top['Importance'], 
                     color='green', alpha=0.7)
    ax2.set_yticks(range(len(best_top)))
    ax2.set_yticklabels(best_top['Feature'])
    ax2.set_xlabel('Feature Importance')
    screen_time_imp = best_importance[best_importance['Feature'] == 'Screen_Time_hrs_day']['Importance'].iloc[0] if 'Screen_Time_hrs_day' in best_importance['Feature'].values else 0
    ax2.set_title(f'AFTER: {best_key}\nScreen_Time reduced to: {screen_time_imp:.1%}')
    ax2.grid(axis='x', alpha=0.3)
    
    for i, (bar, value) in enumerate(zip(bars2, best_top['Importance'])):
        ax2.text(value + 0.01, i, f'{value:.1%}', va='center', fontsize=10)

plt.tight_layout()
plt.savefig('before_after_redistributed.png', dpi=300, bbox_inches='tight')
plt.show()

# Save the best redistributed model and scaler
if best_models:
    print(f"\nüíæ Step 7: Saving the best redistributed model...")
    
    # Save model and scaler
    joblib.dump(best_result['model'], 'smartphone_addiction_model_redistributed.pkl')
    joblib.dump(best_result['scaler'], 'feature_scaler_redistributed.pkl')
    
    # Save feature names for the app
    feature_info = {
        'features': redistributed_features,
        'model_type': best_key,
        'screen_time_importance': screen_time_imp
    }
    joblib.dump(feature_info, 'model_info_redistributed.pkl')
    
    print("‚úÖ Saved redistributed model as 'smartphone_addiction_model_redistributed.pkl'")
    print("‚úÖ Saved feature scaler as 'feature_scaler_redistributed.pkl'")
    print("‚úÖ Saved model info as 'model_info_redistributed.pkl'")

# Summary
print("\nüìã FINAL REDISTRIBUTED SUMMARY:")
print("=" * 50)
print(f"‚úÖ Original Screen_Time importance: 98.4%")
if best_models:
    print(f"‚úÖ New Screen_Time importance: {screen_time_imp:.1%}")
    print(f"‚úÖ Improvement: {98.4 - screen_time_imp*100:.1f} percentage points")
    print(f"‚úÖ Best model: {best_key}")
    print(f"‚úÖ Accuracy maintained: {best_result['accuracy']:.3f}")
    print(f"‚úÖ Features redistributed: {len(redistributed_features)} (no Screen_Time_Ratio)")
else:
    print("‚ö†Ô∏è No significant improvement achieved with current approaches")

print("\nüîç TOP 5 FEATURES IN BEST REDISTRIBUTED MODEL:")
if best_models:
    for i, (_, row) in enumerate(best_result['feature_importance'].head(5).iterrows()):
        print(f"{i+1}. {row['Feature']:25} {row['Importance']:6.1%}")

# Show how importance was redistributed
if best_models:
    print("\nüìä IMPORTANCE REDISTRIBUTION ANALYSIS:")
    print("=" * 45)
    
    # Calculate total importance of non-Screen_Time features
    non_screen_features = best_result['feature_importance'][best_result['feature_importance']['Feature'] != 'Screen_Time_hrs_day']
    total_other_importance = non_screen_features['Importance'].sum()
    
    print(f"Screen_Time_hrs_day: {screen_time_imp:.1%}")
    print(f"All other features: {total_other_importance:.1%}")
    print(f"Total: {screen_time_imp + total_other_importance:.1%}")
    
    print(f"\nTop 5 non-Screen_Time features:")
    for i, (_, row) in enumerate(non_screen_features.head(5).iterrows()):
        print(f"{i+1}. {row['Feature']:25} {row['Importance']:6.1%}")

print("\nüöÄ NEXT STEPS:")
print("=" * 20)
print("1. Update Streamlit app to use the redistributed model")
print("2. Test with real user data")
print("3. Deploy the improved version")

print("\n" + "="*70)
print("üéâ REDISTRIBUTED FEATURE BALANCING COMPLETE!")
print("="*70)
